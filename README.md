# (WIP) Ã¦
Code base for training GPT-like models on TPUs with support for parallelization and scaling on JAX.

## Special Thanks
- [Phil Wang](https://github.com/lucidrains) for the [PaLM-jax](https://github.com/lucidrains/PaLM-jax)
- [Hugging Face](https://huggingface.co/) for the [transformers](https://github.com/huggingface/transformers)